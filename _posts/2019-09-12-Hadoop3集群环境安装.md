---
layout: "post"
title: "Hadoop3集群环境安装"
date: "2019-09-12 11:52"
categories: Hadoop
description: Hadoop3集群环境安装
tags: Hadoop
---

* content
{:toc}

<div class="postImg" style="background-image:url(http://carforeasy.cn/Hadoop3集群环境安装-a0c83b02.png)" ></div>
> “本文介绍Hadoop 3集群环境搭建，采用虚拟机搭建Hadoop集群，配置好一台机器后直接进行虚拟机复制，可加快搭建速度。”






## 1. 环境

### 1.1 设置主机名，配置hosts文件

  准备3台虚拟机(CentOS 7)，一台 master，两台 slaver。master 作为NameNode、DataNode、ResourceManager、NodeManager，slave 均作为DataNode、NodeManager。

  ```
  master : master.test.com
  slave1 : node1.test.com
  slave2 : node2.test.com
  ```

  根据IP地址配置好/etc/hosts文件。

### 1.2 设置环境变量
  系统中安装jdk，将下载的hadoop 压缩包解压至/opt/hadoop，之后配置环境变量：

  ```
  mkdir -p /opt/hadoop
  tar --strip-components=1 -xvf hadoop-3.1.2.tar.gz -C /opt/hadoop/
  ```

  + /etc/profile中添加

  ```
  #java
  export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk
  export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
  export PATH=$PATH:$JAVA_HOME/bin

  #hadoop
  export HADOOP_HOME=/opt/hadoop
  export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
  ```

### 1.3 配置时间同步
  安装NTP

  ```
  yum -y install ntp
  systemctl enable ntpd
  systemctl start ntpd
  timedatectl set-timezone Asia/Shanghai
  timedatectl set-ntp yes
  ```

## 2. Hadoop配置



### 2.1 新建用户及用户组

  为hadoop组件建立单独的用户组hadoop, 创建用户hdfs, yarn, mapred

  ```
  groupadd hadoop
  useradd hdfs -g hadoop
  useradd yarn -g hadoop
  useradd mapred -g hadoop

  ```


### 2.2 创建相关目录

+ 数据存放目录

  - NameNode 数据存放目录： /data/hadoop/namenode
  - DataNode 数据存放目录： /data/hadoop/datanode
  - 临时数据存放目录： /data/hadoop/tmp
  - HADOOP_MAPRED_HOME :

  ```
  mkdir -p /data/hadoop/namenode
  mkdir -p /data/hadoop/datanode
  mkdir -p /data/hadoop/tmp


  chown -R hdfs:hadoop /opt/hadoop
  chown -R hdfs:hadoop /data/hadoop
  ```

+ 新建日志目录

  ```
  mkdir /var/log/hadoop
  chown hdfs:hadoop /var/log/hadoop
  chmod -R 770 /var/log/hadoop

  ```

+ 新建pid目录

  ```
  mkdir /var/run/hadoop
  chown hdfs:hadoop /var/run/hadoop
  chmod -R 770 /var/run/hadoop

  ```

### 2.1 配置文件

共需要配置/opt/hadoop/etc/hadoop/下的6个文件，分别是
+ hadoop-env.sh
+ core-site.xml
+ hdfs-site.xml
+ yarn-site.xml
+ mapred-site.xml
+ workers


#### 2.1.1 hadoop-env.sh

```
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk

export HDFS_NAMENODE_USER="hdfs"
export HDFS_DATANODE_USER="hdfs"
export HDFS_SECONDARYNAMENODE_USER="hdfs"
export YARN_RESOURCEMANAGER_USER="yarn"
export YARN_NODEMANAGER_USER="yarn"

export HADOOP_PID_DIR=/var/run/hadoop
export HADOOP_LOG_DIR=/var/log/hadoop

```

#### 2.1.2 core-site.xml

```xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://master.test.com:8020</value>
        <description>指定默认的访问地址以及端口号</description>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/hadoop/tmp</value>
        <description>其它临时目录的父目录，会被其它临时目录用到</description>
    </property>
    <property>
         <name>io.file.buffer.size</name>
         <value>131072</value>
        <description>在序列中使用的缓冲区大小</description>
    </property>
</configuration>

```


#### 2.1.3 hdfs-site.xml


```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
        <description>副本数，HDFS存储时的备份数量</description>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/data/hadoop/namenode</value>
        <description>namenode临时文件所存放的目录</description>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/data/hadoop/datanode</value>
        <description>datanode临时文件所存放的目录</description>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>master.test.com:50070</value>
        <description>hdfs web 地址</description>
    </property>
</configuration>
```


#### 2.1.4 yarn-site.xml

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        <description>nomenodeManager获取数据的方式是shuffle</description>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master.test.com</value>
        <description>指定Yarn的ResourceManager的地址</description>
    </property>

    <property>
          <name>yarn.resourcemanager.webapp.address</name>
          <value>master.test.com:8088</value>
        <description>配置 yarn 外部可访问，(外网IP:端口)</description>
    </property>

    <property>
        <name>yarn.nodemanager.env-whitelist</name>
    <value> JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ</value>
        <description>容器可能会覆盖的环境变量，而不是使用NodeManager的默认值</description>
    </property>

    <property>
       <name>yarn.nodemanager.vmem-check-enabled</name>
       <value>false</value>
        <description>关闭内存检测，虚拟机需要，不配会报错</description>
    </property>

    <property>
       <name>yarn.nodemanager.local-dirs</name>
       <value>/tmp/nm-local-dir</value>
    </property>


</configuration>
```

#### 2.1.5 mapred-site.xml

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
        <description>告诉hadoop以后MR(Map/Reduce)运行在YARN上</description>
    </property>

   <property>
        <name>mapreduce.admin.user.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        <description>可以设置AM【AppMaster】端的环境变量，如果上面缺少配置，
        可能会造成mapreduce失败</description>
   </property>

   <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        <description>可以设置AM【AppMaster】端的环境变量，如果上面缺少配置，
        可能会造成mapreduce失败</description>
   </property>
</configuration>
```
#### 2.1.6 workers

```
master.test.com
node1.test.com
node2.test.com
```

## 3. 集群操作测试

### 3.1 启动集群

+ 1. 格式化namenode

  ```
  su hdfs -c 'hdfs namenode -format'
  ```

+ 2.启动namenode

  ```
  su hdfs -c 'hdfs --daemon start namenode'
  ```

+ 3. 分别启动每个datanode结点

  ```
  su hdfs -c 'hdfs --daemon start datanode'
  ```

+ 4. 启动resourcemanager

  ```
  su yarn -c 'yarn --daemon start resourcemanager'
  ```

+ 5. 分别启动每个nodemanager节点

  ```
  su yarn -c 'yarn --daemon start nodemanager'
  ```

+ 6. 启动historyserver

  ```
  su mapred -c 'mr-jobhistory-daemon.sh start historyserver'

  ```


### 3.2 停止集群

+ 1.停止namenode

  ```
  su hdfs -c 'hdfs --daemon stop namenode'
  ```

+ 2. 停止datanode结点

  ```
  su hdfs -c 'hdfs --daemon stop datanode'
  ```

+ 3. 停止resourcemanager

  ```
  su yarn -c 'yarn --daemon stop resourcemanager'
  ```

+ 5. 停止nodemanager节点

  ```
  su yarn -c 'yarn --daemon stop nodemanager'
  ```

+ 6. 停止historyserver

  ```
  su mapred -c 'mr-jobhistory-daemon.sh stop historyserver'

  ```

### 3.3 WebUI访问  

+ HDFS UI
[http://master.test.com:50070/](http://master.test.com:50070/)

![](http://carforeasy.cn/Hadoop3集群环境安装-076b5f38.png)

+ YARN UI
[http://master.test.com:8088/](http://master.test.com:8088/)
![](http://carforeasy.cn/Hadoop3集群环境安装-0f4623d4.png)

### 3.4 其他操作指令

+ 查看hdfs目录

  ```
  su hdfs -c 'hdfs dfs -ls /'
  ```

+ 新建hdfs目录

  ```
  su hdfs -c 'hdfs dfs -mkdir PATH'
  ```

+ 修改文件所有者

  ```
  su hdfs -c 'hdfs dfs -chown OWNER:GROUP PATH'
  ```

+ 修改文件权限

  ```
  su hdfs -c 'hdfs dfs -chmod 644 PATH'
  ```

+ webhdfs 操作

  ```
  curl -i  "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=LISTSTATUS"
  curl -i -X DELETE "http://<host>:<port>/webhdfs/v1/<path>?op=DELETE
                                [&recursive=<true |false>]"                           
  curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=CREATE
                      [&overwrite=<true |false>][&blocksize=<LONG>][&replication=<SHORT>]
                      [&permission=<OCTAL>][&buffersize=<INT>][&noredirect=<true|false>]"                   
  curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETOWNER
                                [&owner=<USER>][&group=<GROUP>]"                          
  curl -i -X PUT "http://<HOST>:<PORT>/webhdfs/v1/<PATH>?op=SETPERMISSION
                                [&permission=<OCTAL>]"     

  ```


## 4. 参考链接

+ [hadoop 3.0 集群部署,超详细](https://juejin.im/post/5cbdbc19f265da03b11f3722)
